{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseEinsum(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 output_shape,\n",
    "                 num_summed_dimensions=1,\n",
    "                 activation=None,\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer=\"glorot_uniform\",\n",
    "                 bias_initializer=\"zeros\",\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 ):\n",
    "        self._output_shape=output_shape\n",
    "        self._num_summed_dimentions=num_summed_dimensions\n",
    "        self._activation = tf.keras.activations.get(activation)\n",
    "        self._use_bias = use_bias\n",
    "        self._kernel_initializer = tf.keras.initializers.get(kernel_initializer)\n",
    "        self._bias_initializer = tf.keras.initializers.get(bias_initializer)\n",
    "        self._kernel_regularizer = tf.keras.regularizers.get(kernel_regularizer)\n",
    "        self._bias_regularizer = tf.keras.regularizers.get(bias_regularizer)\n",
    "        self._kernel_constraint = tf.keras.constraints.get(kernel_constraint)\n",
    "        self._bias_constraint = tf.keras.constraints.get(bias_constraint)\n",
    "        self._einsum_string = None\n",
    "        self._CHR_IDX = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\"]\n",
    "        super(DenseEinsum,self).__init__()\n",
    "\n",
    "    def _build_einsum_string(self, free_input_dims, bound_dims, output_dims):\n",
    "        input_str = \"\"\n",
    "        kernel_str = \"\"\n",
    "        output_str = \"\"\n",
    "        letter_offset = 0\n",
    "        for i in range(free_input_dims):\n",
    "            char = self._CHR_IDX[i + letter_offset]\n",
    "            input_str += char\n",
    "            output_str += char\n",
    "\n",
    "        letter_offset += free_input_dims\n",
    "        for i in range(bound_dims):\n",
    "            char = self._CHR_IDX[i + letter_offset]\n",
    "            input_str += char\n",
    "            kernel_str += char\n",
    "\n",
    "        letter_offset += bound_dims\n",
    "        for i in range(output_dims):\n",
    "            char = self._CHR_IDX[i + letter_offset]\n",
    "            kernel_str += char\n",
    "            output_str += char\n",
    "\n",
    "        return input_str + \",\" + kernel_str + \"->\" + output_str\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        input_shape = tf.TensorShape(input_shape)\n",
    "        input_rank = input_shape.rank\n",
    "        self._einsum_string=self._build_einsum_string(input_rank-self._num_summed_dimentions,\n",
    "                                                      self._num_summed_dimentions,\n",
    "                                                      len(self._output_shape))  # \"BTF,FCD->BTCD\"\n",
    "\n",
    "        self._kernel_shape = (input_shape[input_rank-self._num_summed_dimentions:].concatenate(self._output_shape))\n",
    "        self._kernel = self.add_weight(\"kernel\",\n",
    "                                       shape=self._kernel_shape,\n",
    "                                       initializer=self._kernel_initializer,\n",
    "                                       regularizer=self._kernel_regularizer,\n",
    "                                       constraint=self._kernel_constraint,\n",
    "                                       dtype=self.dtype,\n",
    "                                       trainable=True)\n",
    "        if self._use_bias:\n",
    "            self._bias = self.add_weight(\"bias\",\n",
    "                                         shape=self._output_shape,\n",
    "                                         initializer=self._bias_initializer,\n",
    "                                         regularizer=self._bias_regularizer,\n",
    "                                         constraint=self._bias_constraint,\n",
    "                                         dtype=self.dtype,\n",
    "                                         trainable=True)\n",
    "        else:\n",
    "            self._bias = None\n",
    "\n",
    "        super(DenseEinsum, self).build(input_shape)\n",
    "\n",
    "    def call(self,inputs):\n",
    "        ret=tf.einsum(self._einsum_string,inputs,self._kernel)\n",
    "        if self._use_bias:\n",
    "            ret += self._bias\n",
    "        if self._activation is not None:\n",
    "            ret = self._activation(ret)\n",
    "        return ret\n",
    "\n",
    "\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self,hidden_size,num_heads,attention_dropout):\n",
    "        if hidden_size%num_heads:\n",
    "            raise ValueError(\"Hidden size ({}) must be divisible by the number of heads ({}).\"\n",
    "                             .format(hidden_size, num_heads))\n",
    "\n",
    "        super(Attention,self).__init__()\n",
    "        self.hidden_size=hidden_size\n",
    "        self.num_heads=num_heads\n",
    "        self.attention_dropout=attention_dropout\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.query_dense_layer = DenseEinsum([self.num_heads,self.hidden_size//self.num_heads])\n",
    "        self.key_dense_layer = DenseEinsum([self.num_heads,self.hidden_size//self.num_heads])\n",
    "        self.value_dense_layer = DenseEinsum([self.num_heads,self.hidden_size//self.num_heads])\n",
    "        self.output_dense_layer = DenseEinsum([self.hidden_size,],num_summed_dimensions=2)\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def forward(self,query_input,source_input,attention_mask,training,cache):\n",
    "        query=self.query_dense_layer(query_input)\n",
    "        key=self.key_dense_layer(source_input)\n",
    "        value=self.value_dense_layer(source_input)\n",
    "\n",
    "        if cache is not None:\n",
    "            key = tf.concat([tf.cast(cache[\"k\"], key.dtype), key], axis=1)\n",
    "            value = tf.concat([tf.cast(cache[\"v\"], value.dtype), value], axis=1)\n",
    "\n",
    "            # Update cache\n",
    "            cache[\"k\"] = key\n",
    "            cache[\"v\"] = value\n",
    "\n",
    "        depth = (self.hidden_size // self.num_heads)\n",
    "        query *= depth ** -0.5\n",
    "        logits = tf.einsum(\"BTNH,BFNH->BNFT\", key, query)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            logits+=attention_mask\n",
    "\n",
    "        weights = tf.nn.softmax(logits, name=\"attention_weights\")\n",
    "        if training:\n",
    "            weights = tf.nn.dropout(weights, rate=self.attention_dropout)\n",
    "        attention_output = tf.einsum(\"BNFT,BTNH->BFNH\", weights, value)\n",
    "\n",
    "        attention_output = self.output_dense_layer(attention_output)\n",
    "        #print('*'*10,attention_output.shape)\n",
    "        return attention_output\n",
    "\n",
    "    def call(self, query_input, source_input,bias,training,cache=None):\n",
    "        return self.forward(query_input,source_input,bias,training,cache)\n",
    "\n",
    "\n",
    "class FeedForwardNetwork(tf.keras.layers.Layer):\n",
    "    def __init__(self,hidden_size,filter_size,relu_dropout):\n",
    "        super(FeedForwardNetwork,self).__init__()\n",
    "        self.hidden_size=hidden_size\n",
    "        self.filter_size=filter_size\n",
    "        self.relu_dropout=relu_dropout\n",
    "        self.filter_dense_layer = tf.keras.layers.Dense(\n",
    "                                                        self.filter_size,\n",
    "                                                        use_bias=True,\n",
    "                                                        activation=tf.nn.relu,\n",
    "                                                        name=\"filter_layer\")\n",
    "        self.output_dense_layer = tf.keras.layers.Dense(\n",
    "            self.hidden_size, use_bias=True, name=\"output_layer\")\n",
    "\n",
    "    def forward(self,x,training):\n",
    "        output = self.filter_dense_layer(x)\n",
    "        if training:\n",
    "            output = tf.nn.dropout(output, rate=self.relu_dropout)\n",
    "        output = self.output_dense_layer(output)\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"hidden_size\": self.hidden_size,\n",
    "            \"filter_size\": self.filter_size,\n",
    "            \"relu_dropout\": self.relu_dropout,\n",
    "        }\n",
    "\n",
    "    def call(self,x,training):\n",
    "        return self.forward(x,training)\n",
    "\n",
    "\n",
    "class EmbeddingLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,embedding_size):\n",
    "        super(EmbeddingLayer,self).__init__()\n",
    "        self.embedding_size=embedding_size\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        with tf.name_scope('embedding'):\n",
    "            self.shared_weights=self.add_weight(name='weights',\n",
    "                                                shape=[input_shape[-1],self.embedding_size],\n",
    "                                                initializer=tf.random_normal_initializer(mean=0.,\n",
    "                                                                                         stddev=self.embedding_size ** -0.5))\n",
    "        super(EmbeddingLayer,self).build(input_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            #'vocab_size':self.vocab_size,\n",
    "            'embedding_size':self.embedding_size\n",
    "        }\n",
    "\n",
    "    def call(self,x):\n",
    "        y=tf.einsum('bsf,fk->bsk',x,self.shared_weights)\n",
    "        return y\n",
    "\n",
    "\n",
    "class PositionEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self,max_len):\n",
    "        super(PositionEncoding, self).__init__()\n",
    "        self.max_len=max_len\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        super(PositionEncoding,self).build(input_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'max_len': self.max_len\n",
    "        }\n",
    "\n",
    "    def call(self,x,masking=True):\n",
    "        E = x.get_shape().as_list()[-1]  # static\n",
    "        batch_size, seq_length = tf.shape(x)[0], tf.shape(x)[1]  # dynamic\n",
    "        with tf.name_scope('position_encode'):\n",
    "            position_ind = tf.tile(tf.expand_dims(tf.range(seq_length), 0), [batch_size, 1])  # => batch_size*seq_length\n",
    "            position_enc = np.array(\n",
    "                [[pos / np.power(10000, (i - i % 2) / E) for i in range(E)] for pos in range(self.max_len)])\n",
    "\n",
    "            position_enc[:, 0::2] = np.sin(position_enc[:, 0::2])\n",
    "            position_enc[:, 1::2] = np.cos(position_enc[:, 1::2])\n",
    "            position_enc = tf.convert_to_tensor(position_enc, tf.float32)  # (maxlen,E)\n",
    "\n",
    "            outputs = tf.nn.embedding_lookup(position_enc, position_ind)\n",
    "            if masking:\n",
    "                outputs = tf.where(tf.equal(x, 0), x, outputs)\n",
    "        return tf.cast(outputs,tf.float32)\n",
    "\n",
    "\n",
    "class SublayerConnection(tf.keras.layers.Layer):\n",
    "    def __init__(self,sublayer,params):\n",
    "        super(SublayerConnection,self).__init__()\n",
    "        self.sublayer=sublayer\n",
    "        self.params=params\n",
    "        self.layer_postprocess_dropout=params['layer_postprocess_dropout']\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.layer_norm=tf.keras.layers.LayerNormalization(epsilon=1e-6, dtype=\"float32\")\n",
    "        super(SublayerConnection,self).build(input_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'params':self.params\n",
    "        }\n",
    "\n",
    "    def call(self,x,*args,**kwargs):\n",
    "        y=self.sublayer(self.layer_norm(x),*args,**kwargs)\n",
    "        if kwargs['training']:\n",
    "            y=tf.nn.dropout(y,rate=self.layer_postprocess_dropout)\n",
    "        return x+y\n",
    "    \n",
    "\n",
    "\n",
    "class Transformer(object):\n",
    "    def __init__(self, custom_model_params):\n",
    "        params.update(custom_model_params)\n",
    "        self.params=params\n",
    "        self.embedding_layer=EmbeddingLayer(embedding_size=self.params['attention_hidden_size'])\n",
    "        self.encoder_stack=EncoderStack(self.params)\n",
    "        self.decoder_stack=DecoderStack(self.params)\n",
    "        self.projection = tf.keras.layers.Dense(units=1)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {}\n",
    "\n",
    "    def __call__(self, x, predict_seq_length, training):\n",
    "        assert isinstance(x, tuple), \"please input both of inputs and targets\"\n",
    "        inputs,targets=x\n",
    "        self.position_encoding_layer = PositionEncoding(max_len=inputs.get_shape().as_list()[1])\n",
    "        self.position_encoding_layer_2 = PositionEncoding(max_len=predict_seq_length)\n",
    "        if training:\n",
    "            src_mask = self.get_src_mask(inputs)  # => batch_size * sequence_length\n",
    "            src_mask = self.get_src_mask_bias(src_mask)  # => batch_size * 1 * 1 * input_sequence_length\n",
    "            memory=self.encoder(encoder_inputs=inputs,mask=src_mask,training=training)\n",
    "\n",
    "            decoder_output = self.decoder(targets,memory,src_mask,training=training,predict_seq_length=predict_seq_length)\n",
    "            outputs=self.projection(decoder_output)\n",
    "\n",
    "            return outputs\n",
    "        else:\n",
    "            src_mask = self.get_src_mask(x)  # => batch_size * sequence_length\n",
    "            src_mask = self.get_src_mask_bias(src_mask)  # => batch_size * 1 * 1 * input_sequence_length\n",
    "\n",
    "            memory = self.encoder(encoder_inputs=x, mask=src_mask, training=training)\n",
    "\n",
    "            decoder_inputs = tf.ones((x.shape[0], 1, 1), tf.int32)\n",
    "\n",
    "            for _ in range(predict_seq_length):\n",
    "                decoder_inputs_update=self.decoder(decoder_inputs,memory,src_mask,training)\n",
    "                decoder_inputs=tf.concat([decoder_inputs,decoder_inputs_update],axis=1)\n",
    "\n",
    "    def encoder(self,encoder_inputs, mask,training):\n",
    "        '''\n",
    "        :param inputs: sequence_inputs, batch_size * sequence_length * feature_dim\n",
    "        :param training:\n",
    "        :return:\n",
    "        '''\n",
    "        with tf.name_scope(\"encoder\"):\n",
    "            src=self.embedding_layer(encoder_inputs)  # batch_size * sequence_length * embedding_size\n",
    "            src+=self.position_encoding_layer(src)\n",
    "\n",
    "            if training:\n",
    "                src=tf.nn.dropout(src,rate=0.01)  # batch_size * sequence_length * attention_hidden_size\n",
    "\n",
    "            return self.encoder_stack(src,mask,training)\n",
    "\n",
    "    def decoder(self,targets,memory,src_mask,training,predict_seq_length):\n",
    "        with tf.name_scope(\"shift_targets\"):\n",
    "            # Shift targets to the right, and remove the last element\n",
    "            decoder_inputs = tf.pad(targets, [[0, 0], [1, 0], [0, 0]])[:, :-1, :]\n",
    "            tgt_mask=self.get_tgt_mask_bias(predict_seq_length)\n",
    "            tgt=self.embedding_layer(decoder_inputs)\n",
    "\n",
    "        with tf.name_scope(\"add_pos_encoding\"):\n",
    "            pos_encoding = self.position_encoding_layer_2(tgt)\n",
    "            tgt += pos_encoding\n",
    "\n",
    "        if training:\n",
    "            tgt = tf.nn.dropout(tgt, rate=self.params[\"layer_postprocess_dropout\"])\n",
    "        with tf.name_scope('decoder'):\n",
    "            logits=self.decoder_stack(tgt,memory,src_mask,tgt_mask,training)  # Todo：mask\n",
    "        return logits\n",
    "\n",
    "    def get_src_mask(self,x,pad=0):\n",
    "        src_mask = tf.reduce_all(tf.math.equal(x, pad),axis=-1)\n",
    "        return src_mask\n",
    "\n",
    "    def get_src_mask_bias(self,mask):\n",
    "        attention_bias = tf.cast(mask, tf.float32)\n",
    "        attention_bias = attention_bias * tf.constant(-1e9, dtype=tf.float32)\n",
    "        attention_bias = tf.expand_dims(tf.expand_dims(attention_bias, 1),1)  # => batch_size * 1 * 1 * input_length\n",
    "        return attention_bias\n",
    "\n",
    "    def get_tgt_mask_bias(self,length):\n",
    "        valid_locs = tf.linalg.band_part(tf.ones([length, length], dtype=tf.float32),-1, 0)\n",
    "        valid_locs = tf.reshape(valid_locs, [1, 1, length, length])\n",
    "        decoder_bias = -1e9 * (1.0 - valid_locs)\n",
    "        return decoder_bias\n",
    "\n",
    "\n",
    "class EncoderStack(tf.keras.layers.Layer):\n",
    "    def __init__(self,params):\n",
    "        super(EncoderStack, self).__init__()\n",
    "        self.params=params\n",
    "        self.layers=[]\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        for _ in range(self.params['n_layers']):\n",
    "            attention_layer=Attention(self.params['attention_hidden_size'],\n",
    "                                      self.params['num_heads'],\n",
    "                                      self.params['attention_dropout'])\n",
    "            feed_forward_layer=FeedForwardNetwork(self.params['ffn_hidden_size'],\n",
    "                                                  self.params['ffn_filter_size'],\n",
    "                                                  self.params['relu_dropout'])\n",
    "            post_attention_layer=SublayerConnection(attention_layer,self.params)\n",
    "            post_feed_forward_layer=SublayerConnection(feed_forward_layer,self.params)\n",
    "            self.layers.append([post_attention_layer,post_feed_forward_layer])\n",
    "        self.output_norm=tf.keras.layers.LayerNormalization(epsilon=1e-6, dtype=\"float32\")\n",
    "        super(EncoderStack,self).build(input_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "        }\n",
    "\n",
    "    def call(self,encoder_inputs, src_mask, training):\n",
    "        for n, layer in enumerate(self.layers):\n",
    "            attention_layer = layer[0]\n",
    "            ffn_layer = layer[1]\n",
    "\n",
    "            with tf.name_scope('layer_{}'.format(n)):\n",
    "                with tf.name_scope('self_attention'):\n",
    "                    encoder_inputs = attention_layer(encoder_inputs,encoder_inputs, src_mask, training=training)\n",
    "                with tf.name_scope('ffn'):\n",
    "                    encoder_inputs = ffn_layer(encoder_inputs, training=training)\n",
    "\n",
    "        return self.output_norm(encoder_inputs)\n",
    "\n",
    "\n",
    "class DecoderStack(tf.keras.layers.Layer):\n",
    "    def __init__(self,params):\n",
    "        super(DecoderStack,self).__init__()\n",
    "        self.params=params\n",
    "        self.layers=[]\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        for _ in range(self.params['n_layers']):\n",
    "            self_attention_layer=Attention(self.params['attention_hidden_size'],\n",
    "                                           self.params['num_heads'],\n",
    "                                           self.params['attention_dropout'])\n",
    "            enc_dec_attention_layer=Attention(self.params['attention_hidden_size'],\n",
    "                                              self.params['num_heads'],\n",
    "                                              self.params['attention_dropout'])\n",
    "            feed_forward_layer=FeedForwardNetwork(self.params['ffn_hidden_size'],\n",
    "                                                  self.params['ffn_filter_size'],\n",
    "                                                  self.params['relu_dropout'])\n",
    "            post_self_attention_layer=SublayerConnection(self_attention_layer,self.params)\n",
    "            post_enc_dec_attention_layer=SublayerConnection(enc_dec_attention_layer,self.params)\n",
    "            post_feed_forward_layer=SublayerConnection(feed_forward_layer,self.params)\n",
    "            self.layers.append([post_self_attention_layer,post_enc_dec_attention_layer,post_feed_forward_layer])\n",
    "        self.output_norm=tf.keras.layers.LayerNormalization(epsilon=1e-6, dtype=\"float32\")\n",
    "        super(DecoderStack,self).build(input_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'params':self.params\n",
    "        }\n",
    "\n",
    "    def call(self, decoder_inputs,encoder_outputs,src_mask,tgt_mask,training,cache=None):\n",
    "        for n, layer in enumerate(self.layers):\n",
    "            self_attention_layer = layer[0]\n",
    "            enc_dec_attention_layer = layer[1]\n",
    "            ffn_layer = layer[2]\n",
    "\n",
    "            #layer_cache = cache[layer_name] if cache is not None else None\n",
    "            with tf.name_scope(\"dec_layer_{}\".format(n)):\n",
    "                with tf.name_scope('self_attention'):\n",
    "                    decoder_inputs = self_attention_layer(decoder_inputs,decoder_inputs,tgt_mask,training=training)\n",
    "                with tf.name_scope('enc_dec_attention'):\n",
    "                    decoder_inputs = enc_dec_attention_layer(decoder_inputs,encoder_outputs,src_mask,training=training)  # Todo: mask??\n",
    "                with tf.name_scope('ffn'):\n",
    "                    decoder_inputs = ffn_layer(decoder_inputs,training=training)\n",
    "        return self.output_norm(decoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    'n_layers':6,\n",
    "    'attention_hidden_size':64*8,\n",
    "    'num_heads':8,\n",
    "    'ffn_hidden_size':64*8,\n",
    "    'ffn_filter_size':64*8,\n",
    "    'attention_dropout':0.1,\n",
    "    'relu_dropout':0.1,\n",
    "    'layer_postprocess_dropout':0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = EncoderStack(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.build(input_shape=(1440,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
