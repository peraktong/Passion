{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer for time-series problem\n",
    "\n",
    "https://arxiv.org/abs/1706.03762 <br>\n",
    "https://www.tensorflow.org/tutorials/text/transformer <br>\n",
    "Pytorch version will come soon <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split \n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Calculate scaled dot product: q k v\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "    # calculate attention weight:\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "    # Attention (q.k.v)\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "\n",
    "def print_out(q, k, v):\n",
    "    temp_out, temp_attn = scaled_dot_product_attention(\n",
    "      q, k, v, None)\n",
    "      \n",
    "    print ('Attention weights are:')\n",
    "    print (temp_attn)\n",
    "    print ('Output is:')\n",
    "    print (temp_out)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # some temporary weights:\n",
    "\n",
    "\n",
    "if True:\n",
    "    np.set_printoptions(suppress=True)\n",
    "\n",
    "    temp_k = tf.constant([[10,0,0],\n",
    "                          [0,10,0],\n",
    "                          [0,0,10],\n",
    "                          [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "    temp_v = tf.constant([[   1,0],\n",
    "                          [  10,0],\n",
    "                          [ 100,5],\n",
    "                          [1000,6]], dtype=tf.float32)  # (4, 2)\n",
    "    temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "    print_out(temp_q, temp_k, temp_v)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multi-head Attention:\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        \n",
    "        # Always use Super to inheriatte and avoid extra code.\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        # sanity check:\n",
    "        assert d_model % self.num_heads == 0\n",
    "        self.depth = d_model // self.num_heads\n",
    "        # Q K W:\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "    def split_heads(self, x, batch_size):\n",
    "        # Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "        \n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        \n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/transpose : perm\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        return output, attention_weights\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "# check our Multi-head attention:\n",
    "\n",
    "# change d to smaller\n",
    "n_d_model=16\n",
    "\n",
    "\n",
    "temp_mha = MultiHeadAttention(d_model=n_d_model, num_heads=8)\n",
    "y = tf.random.uniform((1, 60, n_d_model))  # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "out.shape, attn.shape\n",
    "    \n",
    "# Point wise feed forward network consists of two fully-connected layers with a ReLU activation in between.\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    # Two FC layers:\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])\n",
    "sample_ffn = point_wise_feed_forward_network(512, 2048)\n",
    "sample_ffn(tf.random.uniform((64, 50, 512))).shape\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    # Here we use a 0.1 dropout rate as default\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        \n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        \n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "        return out2\n",
    "        \n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                              np.arange(d_model)[np.newaxis, :],\n",
    "                              d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "#pos_encoding = positional_encoding(50, 512)\n",
    "#print (pos_encoding.shape)\n",
    "\n",
    "\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.d_model)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "    \n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "        return x  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    \n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                 look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "    \n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                               input_vocab_size, pe_input, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                               target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "        \n",
    "    def call(self, inp, tar, training, enc_padding_mask, \n",
    "           look_ahead_mask, dec_padding_mask):\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "    \n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "        \n",
    "        \n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 36, 8000])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_transformer = Transformer(\n",
    "    num_layers=2, d_model=16, num_heads=8, dff=2048, \n",
    "    input_vocab_size=8500, target_vocab_size=8000, \n",
    "    pe_input=10000, pe_target=6000)\n",
    "\n",
    "temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\n",
    "temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "fn_out, _ = sample_transformer(temp_input, temp_target, training=False, \n",
    "                               enc_padding_mask=None, \n",
    "                               look_ahead_mask=None,\n",
    "                               dec_padding_mask=None)\n",
    "\n",
    "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder (Encoder)            multiple                  273504    \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            multiple                  267744    \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             multiple                  136000    \n",
      "=================================================================\n",
      "Total params: 677,248\n",
      "Trainable params: 677,248\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sample_transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Optimizor\n",
    "d_model = 16\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "    \n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "        \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "    \n",
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)\n",
    "# Learning rate curve:\n",
    "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
    "\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and metric\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "  \n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare data 0.00 percent\n",
      "Prepare data 24.01 percent\n",
      "Prepare data 48.03 percent\n",
      "Prepare data 72.04 percent\n",
      "Prepare data 96.06 percent\n",
      "Finish preparing data\n"
     ]
    }
   ],
   "source": [
    "# Load data:\n",
    "import pandas as pd\n",
    "root_path = \"Data/Ant_202007/fusion.csv\"\n",
    "df = pd.read_csv(root_path)\n",
    "names_array = df.keys()[2:]\n",
    "names_array = list(names_array)\n",
    "df.head()\n",
    "\n",
    "from sklearn import preprocessing\n",
    "# Prepare data:\n",
    "index_name = 0\n",
    "n_bins = 10000\n",
    "min_val = np.nanmin(df[names_array[index_name]])\n",
    "max_val = np.nanmax(df[names_array[index_name]])\n",
    "n_per = (max_val-min_val)/n_bins\n",
    "df[names_array[index_name]] = (df[names_array[index_name]]-min_val)//n_per\n",
    "\n",
    "\n",
    "delta_t = 60*2\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "min_max_scaler = preprocessing.StandardScaler()\n",
    "np_scaled = min_max_scaler.fit_transform(df[names_array])\n",
    "\n",
    "df_scaled = pd.DataFrame(np_scaled,columns=names_array)\n",
    "\n",
    "\n",
    "X = np.zeros((df_scaled.shape[0]-delta_t,delta_t,5),dtype=float)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "X = np.zeros((df.shape[0]-delta_t,delta_t,5),dtype=float)\n",
    "\n",
    "\n",
    "# \n",
    "y = df[names_array[index_name]][delta_t:]\n",
    "    \n",
    "for i in range(len(y)):\n",
    "    if i%10000==0:\n",
    "        print(\"Prepare data %.2f percent\"%(100*i/len(y)))\n",
    "    X[i,:,:] = df[i:i+delta_t][names_array].values\n",
    "\n",
    "print(\"Finish preparing data\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we only use one dimension in x\n",
    "X = X[:,:,index_name]\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train,y_train))\n",
    "\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024*8).batch(BATCH_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyper parameteres:\n",
    "\n",
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "\n",
    "input_vocab_size = int(max(df[names_array[index_name]]))+2\n",
    "target_vocab_size = int(max(df[names_array[index_name]]))+2\n",
    "dropout_rate = 0.1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer\n",
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, \n",
    "                          pe_input=input_vocab_size, \n",
    "                          pe_target=target_vocab_size,\n",
    "                          rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MASK\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "def create_masks(inp, tar):\n",
    "    # Encoder padding mask\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "    # Used in the 2nd attention block in the decoder.\n",
    "    # This padding mask is used to mask the encoder outputs.\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "    # Used in the 1st attention block in the decoder.\n",
    "    # It is used to pad and mask future tokens in the input received by \n",
    "    # the decoder.\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# check point\n",
    "checkpoint_path = \"./TFT/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')\n",
    "EPOCHS = 20\n",
    "\n",
    "# Set to be int64\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "    \n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp, \n",
    "                                     True, \n",
    "                                     enc_padding_mask, \n",
    "                                     combined_mask, \n",
    "                                     dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "  \n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss nan Accuracy 0.0000\n",
      "Epoch 1 Batch 200 Loss nan Accuracy 0.0000\n",
      "Epoch 1 Batch 400 Loss nan Accuracy 0.0000\n",
      "Epoch 1 Loss nan Accuracy 0.0000\n",
      "Time taken for 1 epoch: 63.91315269470215 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss nan Accuracy 0.0000\n",
      "Epoch 2 Batch 200 Loss nan Accuracy 0.0000\n",
      "Epoch 2 Batch 400 Loss nan Accuracy 0.0000\n",
      "Epoch 2 Loss nan Accuracy 0.0000\n",
      "Time taken for 1 epoch: 49.487326860427856 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss nan Accuracy 0.0000\n",
      "Epoch 3 Batch 200 Loss nan Accuracy 0.0000\n",
      "Epoch 3 Batch 400 Loss nan Accuracy 0.0000\n",
      "Epoch 3 Loss nan Accuracy 0.0000\n",
      "Time taken for 1 epoch: 49.491544008255005 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss nan Accuracy 0.0000\n",
      "Epoch 4 Batch 200 Loss nan Accuracy 0.0000\n",
      "Epoch 4 Batch 400 Loss nan Accuracy 0.0000\n",
      "Epoch 4 Loss nan Accuracy 0.0000\n",
      "Time taken for 1 epoch: 49.54981732368469 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss nan Accuracy 0.0000\n",
      "Epoch 5 Batch 200 Loss nan Accuracy 0.0000\n",
      "Epoch 5 Batch 400 Loss nan Accuracy 0.0000\n",
      "Saving checkpoint for epoch 5 at ./TFT/train/ckpt-1\n",
      "Epoch 5 Loss nan Accuracy 0.0000\n",
      "Time taken for 1 epoch: 50.75110673904419 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss nan Accuracy 0.0000\n",
      "Epoch 6 Batch 200 Loss nan Accuracy 0.0000\n",
      "Epoch 6 Batch 400 Loss nan Accuracy 0.0000\n",
      "Epoch 6 Loss nan Accuracy 0.0000\n",
      "Time taken for 1 epoch: 49.4175705909729 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss nan Accuracy 0.0000\n",
      "Epoch 7 Batch 200 Loss nan Accuracy 0.0000\n",
      "Epoch 7 Batch 400 Loss nan Accuracy 0.0000\n",
      "Epoch 7 Loss nan Accuracy 0.0000\n",
      "Time taken for 1 epoch: 49.530492067337036 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss nan Accuracy 0.0000\n",
      "Epoch 8 Batch 200 Loss nan Accuracy 0.0000\n",
      "Epoch 8 Batch 400 Loss nan Accuracy 0.0000\n",
      "Epoch 8 Loss nan Accuracy 0.0000\n",
      "Time taken for 1 epoch: 49.49874234199524 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss nan Accuracy 0.0000\n",
      "Epoch 9 Batch 200 Loss nan Accuracy 0.0000\n",
      "Epoch 9 Batch 400 Loss nan Accuracy 0.0000\n",
      "Epoch 9 Loss nan Accuracy 0.0000\n",
      "Time taken for 1 epoch: 49.51668858528137 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss nan Accuracy 0.0000\n",
      "Epoch 10 Batch 200 Loss nan Accuracy 0.0000\n",
      "Epoch 10 Batch 400 Loss nan Accuracy 0.0000\n",
      "Saving checkpoint for epoch 10 at ./TFT/train/ckpt-2\n",
      "Epoch 10 Loss nan Accuracy 0.0000\n",
      "Time taken for 1 epoch: 50.562493324279785 secs\n",
      "\n",
      "Epoch 11 Batch 0 Loss nan Accuracy 0.0000\n",
      "Epoch 11 Batch 200 Loss nan Accuracy 0.0000\n",
      "Epoch 11 Batch 400 Loss nan Accuracy 0.0000\n",
      "Epoch 11 Loss nan Accuracy 0.0000\n",
      "Time taken for 1 epoch: 49.39722919464111 secs\n",
      "\n",
      "Epoch 12 Batch 0 Loss nan Accuracy 0.0000\n",
      "Epoch 12 Batch 200 Loss nan Accuracy 0.0000\n",
      "Epoch 12 Batch 400 Loss nan Accuracy 0.0000\n",
      "Epoch 12 Loss nan Accuracy 0.0000\n",
      "Time taken for 1 epoch: 49.524181842803955 secs\n",
      "\n",
      "Epoch 13 Batch 0 Loss nan Accuracy 0.0000\n",
      "Epoch 13 Batch 200 Loss nan Accuracy 0.0000\n",
      "Epoch 13 Batch 400 Loss nan Accuracy 0.0000\n",
      "Epoch 13 Loss nan Accuracy 0.0000\n",
      "Time taken for 1 epoch: 49.519803285598755 secs\n",
      "\n",
      "Epoch 14 Batch 0 Loss nan Accuracy 0.0000\n",
      "Epoch 14 Batch 200 Loss nan Accuracy 0.0000\n",
      "Epoch 14 Batch 400 Loss nan Accuracy 0.0000\n",
      "Epoch 14 Loss nan Accuracy 0.0000\n",
      "Time taken for 1 epoch: 49.523255825042725 secs\n",
      "\n",
      "Epoch 15 Batch 0 Loss nan Accuracy 0.0000\n",
      "Epoch 15 Batch 200 Loss nan Accuracy 0.0000\n",
      "Epoch 15 Batch 400 Loss nan Accuracy 0.0000\n",
      "Saving checkpoint for epoch 15 at ./TFT/train/ckpt-3\n",
      "Epoch 15 Loss nan Accuracy 0.0000\n",
      "Time taken for 1 epoch: 50.69895625114441 secs\n",
      "\n",
      "Epoch 16 Batch 0 Loss nan Accuracy 0.0000\n",
      "Epoch 16 Batch 200 Loss nan Accuracy 0.0000\n",
      "Epoch 16 Batch 400 Loss nan Accuracy 0.0000\n",
      "Epoch 16 Loss nan Accuracy 0.0000\n",
      "Time taken for 1 epoch: 49.42813515663147 secs\n",
      "\n",
      "Epoch 17 Batch 0 Loss nan Accuracy 0.0000\n",
      "Epoch 17 Batch 200 Loss nan Accuracy 0.0000\n",
      "Epoch 17 Batch 400 Loss nan Accuracy 0.0000\n",
      "Epoch 17 Loss nan Accuracy 0.0000\n",
      "Time taken for 1 epoch: 49.52993059158325 secs\n",
      "\n",
      "Epoch 18 Batch 0 Loss nan Accuracy 0.0000\n",
      "Epoch 18 Batch 200 Loss nan Accuracy 0.0000\n",
      "Epoch 18 Batch 400 Loss nan Accuracy 0.0000\n",
      "Epoch 18 Loss nan Accuracy 0.0000\n",
      "Time taken for 1 epoch: 49.478824853897095 secs\n",
      "\n",
      "Epoch 19 Batch 0 Loss nan Accuracy 0.0000\n",
      "Epoch 19 Batch 200 Loss nan Accuracy 0.0000\n",
      "Epoch 19 Batch 400 Loss nan Accuracy 0.0000\n",
      "Epoch 19 Loss nan Accuracy 0.0000\n",
      "Time taken for 1 epoch: 49.515220642089844 secs\n",
      "\n",
      "Epoch 20 Batch 0 Loss nan Accuracy 0.0000\n",
      "Epoch 20 Batch 200 Loss nan Accuracy 0.0000\n",
      "Epoch 20 Batch 400 Loss nan Accuracy 0.0000\n",
      "Saving checkpoint for epoch 20 at ./TFT/train/ckpt-4\n",
      "Epoch 20 Loss nan Accuracy 0.0000\n",
      "Time taken for 1 epoch: 50.56269097328186 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "  \n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    # inp -> portuguese, tar -> english\n",
    "    set\n",
    "        tar = tf.reshape(tar,(-1,1))\n",
    "        # convert to int64\n",
    "        inp = tf.cast(inp,dtype=tf.int64)\n",
    "        tar = tf.cast(tar,dtype=tf.int64)\n",
    "        \n",
    "\n",
    "        train_step(inp, tar)\n",
    "    \n",
    "        if batch % 200 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, batch, \n",
    "                                                                          train_loss.result(), train_accuracy.result()))\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                         ckpt_save_path))\n",
    "        \n",
    "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))\n",
    "\n",
    "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test,y_test))\n",
    "\n",
    "test_dataset = train_dataset.shuffle(buffer_size=1024*8).batch(BATCH_SIZE)\n",
    "\n",
    "ans = []\n",
    "\n",
    "for batch, (inp, tar) in enumerate(test_dataset):\n",
    "    tar = tf.reshape(tar,(-1,1))\n",
    "    # convert to int64\n",
    "    inp = tf.cast(inp,dtype=tf.int64)\n",
    "    tar = tf.cast(tar,dtype=tf.int64)\n",
    "    fn_out, _ = sample_transformer(inp, tar, training=False, \n",
    "                               enc_padding_mask=None, \n",
    "                               look_ahead_mask=None,\n",
    "                               dec_padding_mask=None)\n",
    "    ans.extend(np.array(fn_out).ravel())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.10617172, -0.07797275,  0.03969153, ...,  0.00255264,\n",
       "        0.09376192,  0.03280225], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\n",
    "temp_target = tf.random.uniform((64, 1), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "\n",
    "fn_out, _ = saple_transformer(temp_input, temp_target, training=True, \n",
    "                               enc_padding_mask=None, \n",
    "                               look_ahead_mask=None,\n",
    "                               dec_padding_mask=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
