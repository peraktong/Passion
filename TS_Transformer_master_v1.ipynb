{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A time series version Transformer for Time series data\n",
    "### Refer to the original transformer, but with difference\n",
    "Reference: https://www.tensorflow.org/tutorials/text/transformer\n",
    "### Split into short and long term model\n",
    "### need do re-think the MAE/MSE/cross entropy loss problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "\n",
    "plot_path = \"plots/\"\n",
    "\n",
    "# Real server data\n",
    "\n",
    "root_path = \"Data/Ant_202007/\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "\n",
    "cif = pd.read_json(root_path+'cif.json', orient='index')\n",
    "paycore = pd.read_json(root_path+'paycore.json', orient='index')\n",
    "paydecision = pd.read_json(root_path+'paydecision.json', orient='index')\n",
    "paydecision2 = pd.read_json(root_path+'paydecision2.json', orient='index')\n",
    "paydecision3 = pd.read_json(root_path+'paydecision3.json', orient='index')\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df[\"time_stamp\"] = cif.index\n",
    "df[\"cif\"] = cif[0].values\n",
    "df[\"paycore\"] = paycore[0].values\n",
    "df[\"paydecision\"] = paydecision[0].values\n",
    "df[\"paydecision2\"] = paydecision2[0].values\n",
    "df[\"paydecision3\"] = paydecision3[0].values\n",
    "\n",
    "# Optional\n",
    "if False:\n",
    "    df.to_csv(root_path+\"fusion.csv\")\n",
    "\n",
    "    \n",
    "# convert time stamp\n",
    "df['time_stamp'] = pd.to_datetime(df['time_stamp'])\n",
    "names_array = np.array(df.keys()[1:],dtype=\"str\")\n",
    "os.listdir(root_path)\n",
    "\n",
    "\n",
    "if True:\n",
    "    \n",
    "    # calculate previous hour high low:\n",
    "    # convert to seconds\n",
    "    temp = df['time_stamp'] - min(df['time_stamp'])\n",
    "    temp = temp.dt.total_seconds().astype(int)\n",
    "    df[\"hours\"] = temp//3600\n",
    "\n",
    "    h_max = max(df[\"hours\"])+1\n",
    "\n",
    "    for n in range(len(names_array)):\n",
    "        df[names_array[n]+\"_open\"] = df[names_array[n]]\n",
    "        df[names_array[n]+\"_close\"] = df[names_array[n]]\n",
    "        df[names_array[n]+\"_max\"] = df[names_array[n]]\n",
    "        df[names_array[n]+\"_min\"] = df[names_array[n]]\n",
    "\n",
    "    for j in range(1,h_max):\n",
    "        mask_j = df[\"hours\"]==j-1\n",
    "        max_val = df[mask_j][names_array].max(axis=0).values\n",
    "        min_val = df[mask_j][names_array].max(axis=0).values\n",
    "        open_val = df[mask_j][names_array].values[0,:]\n",
    "        close_val = df[mask_j][names_array].values[-1,:]\n",
    "        mask_i = df[\"hours\"]==j\n",
    "        r = df[mask_i][names_array].shape[0]\n",
    "        df.loc[mask_i,[r+\"_open\" for r in names_array]] = np.tile(open_val,(r,1))\n",
    "        df.loc[mask_i,[r+\"_close\" for r in names_array]] = np.tile(close_val,(r,1))\n",
    "\n",
    "        df.loc[mask_i,[r+\"_max\" for r in names_array]] = np.tile(max_val,(r,1))\n",
    "        df.loc[mask_i,[r+\"_min\" for r in names_array]] = np.tile(min_val,(r,1))\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale dot attention:\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "    # Dimension of k\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "    # calculate attention weight:\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    return output, attention_weights\n",
    "\n",
    "# Multi-head Attention:\n",
    "# This is what we use\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        \n",
    "        # Always use Super to inheriatte and avoid extra code.\n",
    "        assert d_model%num_heads==0\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        # sanity check:\n",
    "        assert d_model % self.num_heads == 0\n",
    "        self.depth = d_model // self.num_heads\n",
    "        # Q K W:\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "    def split_heads(self, x, batch_size):\n",
    "        # Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "        \n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        \n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/transpose : perm\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        return output, attention_weights\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 43, 512)\n",
      "(64, 50, 512)\n"
     ]
    }
   ],
   "source": [
    "## Encoder decoder for Time series:\n",
    "\n",
    "# pointwise feed forward network\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    # Two FC layers:\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])\n",
    "\n",
    "# Change embedding since it's not int anymore:\n",
    "class EmbeddingLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,embedding_size):\n",
    "        super(EmbeddingLayer,self).__init__()\n",
    "        self.embedding_size=embedding_size\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        with tf.name_scope('embedding'):\n",
    "            self.shared_weights=self.add_weight(name='weights',\n",
    "                                                shape=[input_shape[-1],self.embedding_size],\n",
    "                                                initializer=tf.random_normal_initializer(mean=0.,\n",
    "                                                                                         stddev=self.embedding_size ** -0.5))\n",
    "        super(EmbeddingLayer,self).build(input_shape)\n",
    "\n",
    "\n",
    "    def call(self,x):\n",
    "        y=tf.einsum('bsf,fk->bsk',x,self.shared_weights)\n",
    "        return y\n",
    "    \n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    # Here we use a 0.1 dropout rate as default\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        \n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        \n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "        return out2\n",
    "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_encoder_layer_output = sample_encoder_layer(tf.random.uniform((64, 43, 512)), False, None)\n",
    "\n",
    "print(sample_encoder_layer_output.shape)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "\n",
    "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
    "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output, \n",
    "    False, None, None)\n",
    "\n",
    "print(sample_decoder_layer_output.shape)  # (batch_size, target_seq_len, d_model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                              np.arange(d_model)[np.newaxis, :],\n",
    "                              d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.d_model)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "    \n",
    "        # adding embedding and position encoding.\n",
    "        #print(\"Check\",x.shape)\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        #x = tf.keras.layers.Dense(self.d_model)(x)\n",
    "        #print(\"check 2\",x.shape)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        #print(\"check 3\",x.shape)\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "        #print(\"check 4\",x.shape)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "        return x  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        #x = tf.keras.layers.Dense(self.d_model)(x)\n",
    "        \n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                 look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "    \n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_seq_size, \n",
    "               output_seq_size, input_delta_t, output_delta_t, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                               input_seq_size, input_delta_t, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                               output_seq_size, output_delta_t, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(output_seq_size)\n",
    "\n",
    "\n",
    "    def call(self, inp, tar, training, enc_padding_mask, \n",
    "           look_ahead_mask, dec_padding_mask):\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "        #print(\"check encoder size\",enc_output.shape)\n",
    "\n",
    "    \n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "        \n",
    "        #print(\"check decoder size\",dec_output.shape)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final output size (8, 3, 1000)\n"
     ]
    }
   ],
   "source": [
    "# We encoder the float32 input to input_seq_size/output_seq_size integers\n",
    "# The output is a sliding time table for different time scale prediction:\n",
    "# Eg: you need to make sure your prediction delta_t<output delta_t and input data delta_t < input_delta_t\n",
    "# For GTX 1060 we can set batch=16 and use 4X batch size for Tesla P40\n",
    "\n",
    "batch = 8\n",
    "\n",
    "sample_transformer = Transformer(\n",
    "    num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
    "    input_seq_size=1000, output_seq_size=1000, \n",
    "    input_delta_t=1440, output_delta_t=240)\n",
    "\n",
    "# input: batch+sequence length\n",
    "# biggest length for in/out put is pe_input,  pe_target\n",
    "temp_input = tf.random.uniform((batch, 720), dtype=tf.int64, minval=0, maxval=1000)\n",
    "temp_target = tf.random.uniform((batch, 3), dtype=tf.int64, minval=0, maxval=1000)\n",
    "\n",
    "#temp_input = tf.cast(temp_input,dtype=tf.float32)\n",
    "#temp_target = tf.cast(temp_target,dtype=tf.float32)\n",
    "\n",
    "fn_out, _ = sample_transformer(temp_input, temp_target, training=False, \n",
    "                               enc_padding_mask=None, \n",
    "                               look_ahead_mask=None,\n",
    "                               dec_padding_mask=None)\n",
    "\n",
    "print(\"final output size\",fn_out.shape)  # (batch_size, tar_seq_len, target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare data 0.00 percent\n",
      "Prepare data 24.37 percent\n",
      "Prepare data 48.74 percent\n",
      "Prepare data 73.10 percent\n",
      "Prepare data 97.47 percent\n"
     ]
    }
   ],
   "source": [
    "# prepare data: fow now I only use 1D data, but it can be extended to multiple channel:\n",
    "# Load data:names_array\n",
    "\n",
    "\n",
    "temp = df[\"cif\"]\n",
    "# Normalize to 0-1000\n",
    "\n",
    "temp = (temp-min(temp))/(max(temp)-min(temp))\n",
    "\n",
    "lower, upper = 0, 999\n",
    "temp = [lower + (upper - lower) * x for x in temp]\n",
    "temp = np.array(temp,dtype=int)\n",
    "delta_t = 720\n",
    "delta_t_out = 3\n",
    "\n",
    "\n",
    "X = np.zeros((temp.shape[0]-delta_t-delta_t_out,delta_t,1),dtype=int)\n",
    "\n",
    "for i in range(delta_t_out):\n",
    "    if i==0:\n",
    "        y = temp[delta_t:-delta_t_out]\n",
    "    else:\n",
    "        y = np.c_[y,temp[delta_t+i:-(delta_t_out-i)]]\n",
    "    \n",
    "    \n",
    "for i in range(y.shape[0]):\n",
    "    if i%10000==0:\n",
    "        print(\"Prepare data %.2f percent\"%(100*i/len(y)))\n",
    "    X[i,:,:] = np.atleast_2d(temp[i:i+delta_t]).T\n",
    "\n",
    "\n",
    "train_dataset_TS = tf.data.Dataset.from_tensor_slices((X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEGCAYAAACtqQjWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxV5bno8d+TOQQSyMCUAAlhMiACRurYqtSKQ6XHaov23qvVc7xt8Xaw57T6Oa2n9dR7jx0OVavH2qpFjxaH2pqqdUStVQRiQQQUSDZjGLLDEEggQMJz/1hvwibuJDvJXtk72c/388kna6/hXc/egTx51/uuZ4mqYowxxkRDUqwDMMYYM3BYUjHGGBM1llSMMcZEjSUVY4wxUWNJxRhjTNSkxDqAWMrPz9fi4uJYh2GMMf3K+++/X6eqBeG2JXRSKS4uprKyMtZhGGNMvyIiWzraZpe/jDHGRI0lFWOMMVFjScUYY0zUWFIxxhgTNZZUjDHGRI2vSUVE5orIehGpEpFbw2xPF5En3fZlIlIcsu02t369iFzcVZsi8raIrHJfO0TkT36+N2OMMZ/k25RiEUkG7gMuArYDK0SkQlXXhex2I7BPVSeIyHzgLuDLIlIGzAemAqOB10RkkjsmbJuqel7Iuf8APOfXezPGGBOenz2V2UCVqgZU9SiwGJjXbp95wCK3/AwwR0TErV+sqkdUdRNQ5drrsk0RyQYuBAZcT+Vo83F+v3wrx1qOxzoUY4wJy8+kUghsC3m93a0Lu4+qNgP1QF4nx0bS5heA11X1QLigROQmEakUkcpgMNitNxRrT7+/jdue/ZCH/rYp1qEYY0xYA3Gg/hrg9x1tVNUHVbVcVcsLCsJWGYhbexuOAvBOVV2MIzHGmPD8TCo1wJiQ10VuXdh9RCQFyAH2dHJsp22KSD7eJbIXovIO4symPY0ArNi8l0NHm2McjTHGfJKfSWUFMFFESkQkDW/gvaLdPhXAdW75KmCJes83rgDmu9lhJcBEYHkEbV4FPK+qTb69qxiqDjYiAk3HjvPm+v516c4Ykxh8SypujORm4GXgI+ApVV0rIneIyBVut4eAPBGpAm4BbnXHrgWeAtYBLwELVLWlozZDTjufTi599WeqSiDYwPwzxpKblcZf1uyKdUjGGPMJvlYpVtUXgRfbrbs9ZLkJuLqDY+8E7oykzZBt5/ci3LgWbDjCwaZmJo0YDIygYtUOmo61kJGaHOvQjDGmzUAcqB+QAkFvPGV8wWDmThtF49EW3t5oA/bGmPhiSaWfaE0qpQVZnF2ax9BBqfz5gx0xjsoYY05mSaWfqA42kJGaxOicTFKTk7js1FG8sm4XDUdsFpgxJn5YUuknAsEGSvIHk5QkAFw5q5CmY8d52QbsjTFxxJJKP1EdbGR8QVbb61ljhzE2dxB/XNn+1h9jjIkdSyr9wJHmFrbvO0Rp/omkIiJ8YWYh71TXsat+QN6WY4zphyyp9ANb9hziuELp8MEnrf+HmYWoQsUH1lsxxsQHSyr9QHVtAwDj809OKiX5WcwYM5Q/vF+DV4jAGGNiy5JKPxCoa71HJesT264uL2L97oOs2ra/r8MyxphPsKTSD1TXNjAyO4Os9E8WQJg3o5BBack8sWxrDCIzxpiTWVLpB6rrGsP2UgAGp6cwb8Zo/rx6BweajvVxZMYYczJLKnFOVQnUNlBaMLjDfa6dPY6mY8f5k00vNsbEmCWVOBdsOMLBI80d9lQATi3KYVphNk8s22oD9saYmLKkEudO1PzquKcCXm/l410H+fvWfX0RljHGhGVJJc5VB9104k56KgDzZoxmSEYKD7+zuQ+iMsaY8CypxLlAsLGtkGRnstJTuGb2WF5as4vt+w71UXTGGHMySypxrrpdIcnOXHd2MQCL3t3sb1DGGNMBSypxLhDseDpxe4VDM7lk2kgWL99mJfGNMTFhSSWONR1zhSS7GKQPdeO5JRw80szTldt8jMwYY8LzNamIyFwRWS8iVSJya5jt6SLypNu+TESKQ7bd5tavF5GLu2pTPHeKyAYR+UhEvunne+sLbYUkI+ypAMwcO4xZY4fy0N82cazluI/RGWPMJ/mWVEQkGbgPuAQoA64RkbJ2u90I7FPVCcBC4C53bBkwH5gKzAXuF5HkLtq8HhgDTFHVU4DFfr23vhJwM7+601MB+Mb5E9i+7zDPrbLHDRtj+pafPZXZQJWqBlT1KN4v+Xnt9pkHLHLLzwBzRETc+sWqekRVNwFVrr3O2vw6cIeqHgdQ1Vof31ufaJ1OXJIfeU8FYM4pwzllVDb3v1FFy3G7GdIY03f8TCqFQOiF/e1uXdh9VLUZqAfyOjm2szZLgS+LSKWI/EVEJoYLSkRucvtUBoPBHr2xvhIINnZYSLIzIsL/uXACgbpGXvhwp0/RGWPMJw2kgfp0oElVy4HfAA+H20lVH1TVclUtLygo6NMAu6s62EDp8O71UlrNnTqSCcMH86slGzluvRVjTB/xM6nU4I1xtCpy68LuIyIpQA6wp5NjO2tzO/CsW/4jML3X7yCGVNWbTpzfvfGUVklJws0XTGDD7gZeXrsrytEZY0x4fiaVFcBEESkRkTS8gfeKdvtUANe55auAJepVRKwA5rvZYSXARGB5F23+CbjALX8G2ODT++oTrYUkuzPzq73Lp4+itCCLX7y6gWabCWaM6QO+JRU3RnIz8DLwEfCUqq4VkTtE5Aq320NAnohUAbcAt7pj1wJPAeuAl4AFqtrSUZuurf8AvigiHwL/D/hHv95bX6iubX3aY896KgApyUn8y8WTqapt4Nm/W1l8Y4z/ujcC3E2q+iLwYrt1t4csNwFXd3DsncCdkbTp1u8HLutlyHEjUOemEw/veVIBuHjqSGaMGcrC1zZwxYzRZKQmRyM8Y4wJayAN1A8o1bVeIclR2Rm9akdE+P7cKeysb+LRpZujEpsxxnTEkkqcCtRFXkiyK2eV5vGZSQXc90Y19YfskcPGGP9YUolTgWBjrwbp2/v+3CkcbDrGwtf69fwFY0ycs6QSh5qOtbBt36FeDdK3VzY6m2s/NZbH3tvC+l0Ho9auMcaEsqQSh7bsOYR2s5BkJL570WSGZKTwo4q19ix7Y4wvLKnEoeoeFpLsyrCsNL77ucksDezhL2vshkhjTPRZUolDgR4WkozEtbPHcsqobH7y/Doa7UFexpgos6QSh6qDjYzK6X4hyUgkJwn/Pm8qO+qb+MUrNmhvjIkuSypxKBBsiPgRwj1RXpzL/zxzHI+8u4mVW/f5dh5jTOKxpBJnWgtJRns8pb3vzZ3MyOwMbv3DhxxttrpgxpjosKQSZ4IHvUKS430YTwk1JCOVn3xhGut3H+SBt6p9PZcxJnFYUokz1UGvkGRva35FYs4pI/j8aaO5d8lG1u044Pv5jDEDnyWVONM6nTiaNz525sdXTGXooDS+/eRKmo619Mk5jTEDlyWVOBMIRqeQZKRys9L4+dWnsWF3Az99aX2fnNMYM3BZUokzgboGxkepkGSkPjOpgOvPLubhdzbx9sZgn53XGDPwWFKJM9U+TyfuyK2XTGHC8MH889MfsLfxaJ+f3xgzMFhSiSNNx1rYvu+w79OJw8lITebu+TPYd+gY31q8kpbjVhvMGNN9llTiyOY9jagSk54KwNTROfz4iqm8vbGOe5dsjEkMxpj+zZJKHAm0TieOQU+l1fwzxnDlrELufn0jb22w8RVjTPf4mlREZK6IrBeRKhG5Ncz2dBF50m1fJiLFIdtuc+vXi8jFXbUpIr8TkU0issp9zfDzvfmhuta/QpKREhHu/MKpTB4xhG8vXknN/sMxi8UY0//4llREJBm4D7gEKAOuEZGydrvdCOxT1QnAQuAud2wZMB+YCswF7heR5Aja/BdVneG+Vvn13vwSqPOvkGR3ZKYlc/9XZtHcovzjokqrZmyMiZifPZXZQJWqBlT1KLAYmNdun3nAIrf8DDBHRMStX6yqR1R1E1Dl2oukzX4rEGyI6aWvUOMLBnPvtTNZv+sA33lyFcdt4N4YEwE/k0ohsC3k9Xa3Luw+qtoM1AN5nRzbVZt3ishqEVkoIunhghKRm0SkUkQqg8H4GTNQVaqDjTEbpA/n/MnD+eHlZbyybjc/f8VujDTGdG0gDdTfBkwBzgByge+H20lVH1TVclUtLygo6Mv4OhU8eISGI81x01Npdf3ZxVz7qbHc/2Y1f3h/e6zDMcbEOT+TSg0wJuR1kVsXdh8RSQFygD2dHNthm6q6Uz1HgEfwLpX1G1VtNb/ip6cC3sD9j6+YyjkT8vj+H1bbjDBjTKf8TCorgIkiUiIiaXgD7xXt9qkArnPLVwFLVFXd+vludlgJMBFY3lmbIjLKfRfgC8AaH99b1LVOJ+6rQpLdkZqcxAP/43QmjRjC1//7fVZt2x/rkIwxccq3pOLGSG4GXgY+Ap5S1bUicoeIXOF2ewjIE5Eq4BbgVnfsWuApYB3wErBAVVs6atO19biIfAh8COQDP/HrvfkhEGwkMzW5zwpJdteQjFR+d8MZ5A9O56uPLKfKTX82xphQ4nUMElN5eblWVlbGOgwArnt4OcGDR3jxW+fFOpRObdnTyBf/613SkpN4+utnUzg0M9YhGWP6mIi8r6rl4bYNpIH6fi1Q19AnD+bqrXF5Wfzuq7M5eKSZax58j531dnOkMeYESypxoLWQpN+PEI6WaYU5PHrDbPY1HmX+g++xq74p1iEZY+KEJZU4EOtCkj0xc+wwFt04mz0NR7nmN++x+4AlFmOMJZW4UF0b+0KSPTFr7DAW3XAGtQeauObB96xOmDHGkko8CMTpPSqROH1cLotumE3w4BGu/q93qQ7arDBjElmXSUVEJonI6yKyxr2eLiI/8D+0xBGoa2R0TgaD0mJbSLKnyotz+f1NZ3K05ThXP7CUNTX1sQ7JGBMjkfRUfoNXAuUYgKquxrvp0ESJ9wjh/nXpq71phTk8/bWzyUxNZv6D77G0ek+sQzLGxEAkSWWQqi5vt85qoUeJqhIINlLaDy99tVeSn8UzXz+LkTkZXPfwcp5b1b4qjzFmoIskqdSJSCmgACJyFbDT16gSSK0rJNnfeyqtRuVk8vT/PosZY4fyrcWruPu1jSTyDbbGJJpIksoC4NfAFBGpAb4NfM3XqBJIdT8epO/IsKw0HrtxNlfOKmThaxu45akPONLcEuuwjDF9IJKRYVXVz4pIFpCkqgddkUcTBfHwXHo/pKck84urT2N8fhY/f2UDNfsOc99XZlEwJOxjbowxA0QkPZU/AKhqo6oedOue8S+kxFIdbCAzNZmRcVpIsjdEhJsvnMi918xkdc1+Pn/v3/j71n2xDssY46MOk4qITBGRLwI5InJlyNf1wMD7DRgjAfe0x6QkiXUovvn8aaN59uvnkJaSxJd/vZTH3tti4yzGDFCd9VQmA5cDQ4HPh3zNAv7J/9ASw0CYThyJstHZ/Pnmczl3Qj4//NMa/vnp1Rw+auMsxgw0HY6pqOpzwHMicpaqLu3DmBJG07EWavYf5ouzimIdSp/IGZTKQ9edwd2vb+Tu1zeyevt+7r12JlNGZsc6NGNMlEQyprJSRBaIyP0i8nDrl++RJYBNdV4hyf5Q8j5akpKE71w0yatyfOgYV/zqHR5dutkuhxkzQESSVB4DRgIXA2/hPRf+YKdHmIi0PUK4n5S8j6ZPTyrgpW+fxzmledz+3Fr+6dH32dt4NNZhGWN6KZKkMkFVfwg0quoi4DLgU/6GlRj6cyHJaMgfnM7D15/B7ZeX8dcNQeb+8q+8/tHuWIdljOmFSJLKMfd9v4hMA3KA4f6FlDiqgw39upBkNIgIN5xbwh8XnE1uVho3LqrklidXsf+Q9VqM6Y8iSSoPisgw4AdABbAOuMvXqBJEoK4xocZTOjN1dA4VN5/LN+dMpOKDHVy08K+8us56Lcb0N10mFVX9raruU9W/qup4VR0O/CWSxkVkroisF5EqEbk1zPZ0EXnSbV8mIsUh225z69eLyMXdaPMeEYn7h3qoKtW1DQk5ntKRtJQkbrloEn9acA55WWn806OVfPP3K6k9aE+VNKa/6DSpiMhZInKViAx3r6eLyBPAO101LCLJwH3AJUAZcI2IlLXb7UZgn6pOABbiekBuv/nAVGAucL+IJHfVpoiUA8O6ftuxV3vwCI1HWxLiHpXumlbo9Vq+NWciL63ZxZxfvMWjSzfTctxmiBkT7zq7o/5nwMPAF4EXROQnwCvAMmBiBG3PBqpUNaCqR4HFwLx2+8wDFrnlZ4A5IiJu/WJVPaKqm4Aq116HbbqE8zPgexHEFnOthSQHWs2vaElLSeI7F03iL98+j+lFOdz+3Fr+4f53WL19f6xDM8Z0orOeymXATFW9BvgcXnXiM1X1blWN5HpEIbAt5PV2ty7sPqraDNQDeZ0c21mbNwMVqtppWX4RuUlEKkWkMhgMRvA2/FHdOp04QWd+Raq0YDD/feOnuHv+DHbWNzHvvnf44Z/W2PRjY+JUZ0mlqTV5qOo+YKOqbu6TqLpJREYDVwP3drWvqj6oquWqWl5QUOB/cB0IBBsYlDYwC0lGm4gwb0Yhr3/3M/yvM8fx+LItfOZnb/DbtwMcbT4e6/CMMSE6SyrjRaSi9Qsoafe6KzXAmJDXRW5d2H1EJAVvuvKeTo7taP1MYAJQJSKbgUEiUhVBjDFTHWykJH9gF5KMtuyMVH48bxovffvTnD5uGD954SMuWvgWL63ZZXfkGxMnOrtBov34xy+62fYKYKJ79koN3sD7te32qQCuA5YCVwFLVFVd0npCRP4TGI03hrMckHBtqupavLv+ARCRBjf4H7cCwQZmju0XcwrizqQRQ/jdV2fz1oYgd76wjq/99/vMLsnl1kumMMs+U2NiqrOCkm/1pmFVbRaRm4GXgWTgYVVdKyJ3AJWqWgE8BDzmehV78ZIEbr+n8O6JaQYWqGoLQLg2exNnLLQWkrzq9MQoJOmXz0wq4JzS83iychsLX93Alfe/y5wpw/nu5yZTNtqKVBoTC5LIlw3Ky8u1srKyz8/70c4DXHL329xzzUyuOG10n59/IGo80szv3t3Mr9+q5kBTM5dNH8V3PjuJCXZzqTFRJyLvq2p5uG2JWx8khk48QthmfkVLVnoKCy6YwP84cxy/fTvAQ3/bxF8+3Mk/zCziGxeU2tRtY/pIJGVaTJS13qNSYnfTR11OZirf/dxk3v7eBdxwTgnPr97BZ//zLRY8/nfW1NTHOjxjBrwueyoi8meg/TWyeqAS+HWE96yYEIFgA4VDMxO6kKTf8gan84PLy/ja+aU8/LdNPLZ0Cy98uJPzJxew4IIJnFGcG+sQjRmQIumpBIAG4Dfu6wDe81Qmudemm6rdc+mN//IHp/O9uVP4260X8i8XT2b19nqufmApX3pgKa+u222lX4yJskj+VD5bVc8Ief1nEVmhqmeISL+beRVrqkog2GAzv/pYTmYqCy6YwA3nlLB4xVZ+89cA//RoJePyBnH92cVcXT6GwenWczSmtyLpqQwWkbGtL9xy66in1cropt0HvEKSVvI+NjLTkvnqOSW89b0L+NW1M8nLSuPHf17HWf/3df79+XVs23so1iEa069F8qfZd4G/iUg13s2HJcA3RCSLE8UgTYTanvaYb0klllKTk7h8+mgunz6alVv38cg7m1n07mYeeWcTnz1lBF85cxznTci3igfGdFOXSUVVXxSRicAUt2p9yOD8L32LbICqrnPTiYfbmEq8mDl2GDPHDuO2S6fw2NItLF6xjVfW7WZMbibzzxjLl8rHUDAkPdZhGtMvRHoR+XSg2O1/moigqo/6FtUAVl1rhSTj1aicTL43dwrf+uxEXl67myeWbeFnL69n4asb+NzUEVw7exxnl+ZZ78WYTkQypfgxoBRYBbS41QpYUumBQJ0388t7bIyJR+kpyVxx2miuOG001cEGfr9sK8/8fTsvfriLMbmZXDmziCtnFTIuz3qbxrQXSU+lHCjTRK7nEkXVtQ2cPs6KHvYXpQWD+cHlZfzzxZN5ee0unq7czj1LNnL36xs5o3gYV84q4tJTR5GTmRrrUI2JC5EklTV4FYA7ffiV6VrTsRZ21B/m6gKbTtzfZKQmM29GIfNmFLKz/jB/XFnDH97fzm3Pfsi/Vazlc2Uj+OKsIs6dmE9qshWqMIkrkqSSD6wTkeXAkdaVqnqFb1ENUJvqGlG1Rwj3d6NyMvnG+RP4+mdKWb29nmf/vp2KD3bw/OqdDBuUytxpI7ns1NGcOT6XFEswJsFEklR+5HcQiaK15pfdTT8wiAinjRnKaWOG8q+XlfHWhiDPr95Bxaod/H75NvKy0pg7bSSXTx/N7JJckm2A3ySASKYU9+q5KuaE1urEdo/KwJOWksRFZSO4qGwETcdaeHN9LX9evZNn/17D48u2UjAknUunjeTiaSOZXWw9GDNwdZhURORvqnquiBzk5IKSAqiq2lOQuqnaFZLMTEuOdSjGRxmpycydNoq500Zx6GgzSz6u5fkPdrJ4xTYWLd1CTmYqc6YM56KyEXx6UgFZVh7GDCCdPfnxXPd9SN+FM7AFrJBkwhmUltJ2537jkWbe3hjklXW7WfJxLc+urCEtJYlzJ+RzUdkI5pwynOFD7P4l079F9CeSiCQDI0L3V9WtfgU1ELUWkry6fEysQzExkpWe0taDaW45zorN+3h13W5eWbeLJR/XIgKnFQ3l/MkFnD95OKcW5tg4jOl3Irn58f8A/wbsBo671QpM9zGuAae1kKT1VAxASnISZ5XmcVZpHj+8/BQ+3nWQV10P5u7XN/LL1zaSm5XGeRPzOX9yAedNLCB/sJWKMfEvkp7Kt4DJqrqnu42LyFzgbiAZ+K2q/ke77el4d+afDuwBvqyqm92224Ab8e7i/6aqvtxZmyLyEN6NmgJsAK5X1YbuxuyX1kKSNp3YtCcinDIqm1NGZfPNORPZ23iUtzcGeWt9kLc2BHlu1Q5E4NTCHM6fVMCnJxVw2pihdj+MiUuRJJVteE967BZ3yew+4CJgO7BCRCpUdV3IbjcC+1R1gojMB+4CviwiZcB8YCowGnhNRCa5Yzpq8zuqesCd+z+Bm4GTklgs2XRiE6ncrLS2Gy2PH1fW7KjnTZdgfvVGFfcsqSIrLZnZJbmcXZrP2RPyOGVkttUkM3EhkqQSAN4UkRc4+ebH/+ziuNlAlaoGAERkMTAPCE0q8zhxH8wzwK/EK4o1D1isqkeATSJS5dqjozZDEooAmXzyEcgxVR1stEKSptuSkoTpRUOZXjSUb86ZyP5DR3m3eg/vVtfxbvUe3lj/EQDDBqVyVmmel2RK8yjJt/pyJjYiSSpb3Vea+4pUIV4vp9V24FMd7aOqzSJSD+S59e+1O7bQLXfYpog8AlyKl7i+Gy4oEbkJuAlg7Nix4XbxRXWwwQpJml4bOiiNS08dxaWnjgJgZ/1hllbv4Z0qL9G8+OEuAEblZHDm+DzKi4cxuziXCcMH27890yc6TSruEtYkVf1KH8XTK6r6VRfzvcCXgUfC7PMg8CBAeXl5n/VmAsFGKyRpom5UTiZXziriyllFqCpb9hzineo63q3aw9sb6/jjyhrA68mUF+cyuziXM0pymTo628ZkjC86TSqq2iIi40QkTVW7++jgGiB0/myRWxdun+0ikgLk4A3Yd3Zsp226mBcD3yNMUomFw0e9QpJfKrDpxMY/IkJxfhbF+Vl85VPj2pLM8s17WbFpLys27+XVdbsByExNZubYoZxRnMsZxblMH5NDdoZVWja9F+mYyjsiUgE0tq6MYExlBTBRRErwfvHPB65tt08FcB2wFLgKWKKq6s71hBtwHw1MBJbjzez6RJtuHKVUVavc8hXAxxG8tz7RWkjSBulNXwpNMl9y90fVHmhixeZ9rNjsJZl7l2zkuIIITBw+mBljhjJz7DBmjBnKpBFD7D4Z022RJJVq95UERHx3vRsjuRl4GW/678OqulZE7gAqVbUCeAh4zA3E78VLErj9nsIbG2kGFqhqC0AHbSYBi0QkGy/xfAB8PdJY/Raos+nEJj4Mz87gsumjuGy6NyZzoOkYq7buZ9U27+vVdbt5qnI7AFlpyUwvGsqMsUOZOcb7bnf8m65IIj97q7y8XCsrK30/z92vbWThaxv46I65VvfLxLXWS2artu1n5dZ9rNy2n3U7DtB83Ps9UTg0k1MLczi1KIdphTlMG51Nnt2UmXBE5H1VLQ+3LZI76gvwxiemAm1/pqjqhVGLcIAL1FkhSdM/hF4y+8JMb8Jl07EW1u6oZ+XW/azctp+1NfW8tHZX2zGjczK8BFOYw6nue8EQSzSJKpLLX48DTwKXA1/DGwMJ+hnUQNM6ndiY/igjNZnTx+Vy+rjctnX1h4+xdkc9a2rqWVNzgDU19bziJgEAjMzOYFphNtMKc7xqASOzKRqWaTdoJoBIkkqeqj4kIt9yz1Z5S0RW+B3YQKGqbAo2Ul6e2/XOxvQTOZmp7kbL/LZ1B5uOsXbHAZdo6vmwpp7XP66l9Qp7Vloyk0cOYcqobE5x36eMHMIQm3U2oESSVI657ztF5DJgB2C/ISPUWkiy1HoqZoAbkpHKmePzOHN8Xtu6Q0eb2bC7gY93HuCjnQf4aNdBnv9gB08sa27bp2hYJlNGZnPKqCFMGZnNlFFDKM7Lspln/VQkSeUnIpKDd4f6vUA28B1foxpATtT8splfJvEMSkthxpihzBgztG2dqrKzvomPdx3go50H+XjXQT7eeYA31tfS4iYEpKckUVowmIkjBjNx+GAmDB/MhOFDGJc3yG7ajHORPE74ebdYD1zgbzgDj1UnNuZkIsLooZmMHprJhVNGtK1vOtZCVW1DW5KpCjZQuXkfz63a0bZParJQkp/VlmQmDvcST0l+FukpNhEmHkQy+2sS8F/ACFWdJiLTgStU9Se+RzcAVAcbyUpLZkS2zYYxpjMZqclts8hCNR5pJhBsZGPtQTbWNrBxdwPrdhzgpTW7cB0bkgTG5XnJprRgMOPzsygpyKIkP4u8rDSre9aHIrn89RvgX4BfA6jqahF5ArCkEoHqYAMlVkjSmBwGxs0AABMqSURBVB7LSk/h1CLv3phQTcda2FTXyMbaBqp2u4RT28Cb62s51nLi/rshGSleksnPoiR/MCUFWYx306YHp0f08FvTDZF8ooNUdXm7X4rNHe1sThYINlJebIUkjYm2jNTktoebhWpuOU7N/sME6hrZFGxkU533tWLzPp77YAeh93sPH5JOSX4W412vZlxeFmNzBzE2dxBZlnB6JJJPrU5ESnHPJxGRq4CdvkY1QBw+2kLN/sN8Kd8KSRrTV1KSkxiX5yWICyafvK3pWAtb9hxiU13DSUnnlbW72dN4cs3c/MFpbQlmrEs24/K818OHpNvVhw5EklQW4JWKnyIiNcAmoF+Uwo+1TXVe/c3S4Tad2Jh4kJHq3SszeeQnyxjWHzrGlr2NbN17iC17DrHNfV+xeR8VH+xoG7/x2klizDAvyYzJHcS43EGMdQmncOighK6eEcnsrwDwWRHJApJU9aCIfBv4pe/R9XNt04nzbeaXMfEuZ1Aq0wd5T9ls72izd0lty57GtmSzda/39W71Hg4dbTlp/9ysNAqHZlI0LJPCoZkUDsukaNigtuWczIF7w2fEFw1VtTHk5S1YUulSIOh9ZCX51lMxpj9LS0lyA/2f/L+sqtQ1HGXr3kNs33eI7fsOs33fYWr2H2bD7oMs+biWI83HTzpmSEbKSUmnaNggCtuWM8ntxzPWejoS1T/fbR+rDlohSWMGOhGhYEg6BUPSwz7dVVXZ03iUGpdotu87FLJ8mGWBvRw8cvLcp4zUJEbnZDIyJ4ORORlty6NCXg8dlBqXiaenSSVx6+V3Q6DOCkkak+hEhPzB6eQPTue0MZ+8tAZegc6afS7h7D9Mzb7D7DzQxK76Jt6r3sPug0faqg20Sk9Jaksyo3IyGdWWdE4sx6LH02FSEZGDhE8eAmT6FtEAoaoEgo18yQpJGmO6kJOZSk5mKmWjs8Nubzmu1DUcYWd9Ezv3H2ZnfRO7DjSxY/9hdtU3sXzTXnYfaGp77k2rtJQkRmZnMDI7gxE5GYwYks7InAxGZGfw6UkFvoztdJhUVDXipzyaT9p1oIlDVkjSGBMFyUnCiGwvGczooLdzPDTx1Dexq/7wieUDTazevp9d9U1t4ztLvvuZvk0qpndaB+mt5pcxpi8kJQnDszMYnp3BaR3cGqeqHDjczK4DTYzJHeRLHJZUfGLViY0x8UZEyBmUSs4g/6Y0+1pDWkTmish6EakSkVvDbE8XkSfd9mUiUhyy7Ta3fr2IXNxVmyLyuFu/RkQeFpGYTgQPWCFJY0wC8i2piEgycB9wCVAGXCMiZe12uxHYp6oTgIXAXe7YMmA+MBWYC9wvIsldtPk4MAU4FW8iwT/69d4i4T1CeHBcTvkzxhi/+NlTmQ1UqWpAVY8Ci4F57faZByxyy88Ac8T7LTwPWKyqR1R1E1Dl2uuwTVV9UR1gOVDk43vrUiDYaNOJjTEJx8+kUghsC3m93a0Lu4+qNuM9CCyvk2O7bNNd9vqfwEvhghKRm0SkUkQqg8FgN99SZFoLSdogvTEm0QzE53LeD/xVVd8Ot1FVH1TVclUtLygo8CWAQF3rIL31VIwxicXP2V81QOjEtiK3Ltw+20UkBcgB9nRxbIdtisi/AQXA/45C/D3WOp3YCkkaYxKNnz2VFcBEESkRkTS8gfeKdvtUANe55auAJW5MpAKY72aHlQAT8cZJOmxTRP4RuBi4RlWPE0PVwQZErJCkMSbx+NZTUdVmEbkZeBlIBh5W1bUicgdQqaoVwEPAYyJSBezFSxK4/Z4C1uE9ZXKBqrYAhGvTnfIBYAuw1M24elZV7/Dr/XUmEGxkdI4VkjTGJB5fb35U1ReBF9utuz1kuQm4uoNj7wTujKRNtz5ubuQM1DVQOtwufRljEs9AHKiPqdZCkuPt0pcxJgFZUomytkKS1lMxxiQgSypRVl3rCklaT8UYk4AsqUTZiXtUrKdijEk8llSizApJGmMSmSWVKLNCksaYRGZJJcoCwUZ72qMxJmFZUomiQ0ebqdl/2MZTjDEJy5JKFG2qczW/rKdijElQllSiqNqeS2+MSXCWVKIoYIUkjTEJzpJKFAWCjRQOzSQj1QpJGmMSkyWVKGqdTmyMMYnKkkqUHD+uNp3YGJPwLKlEya4DTRw+1mI9FWNMQrOkEiWtjxC2QpLGmERmSSVKWgtJWsl7Y0wis6QSJdW1DWSlJTN8iBWSNMYkLksqURKoa6R0uBWSNMYkNl+TiojMFZH1IlIlIreG2Z4uIk+67ctEpDhk221u/XoRubirNkXkZrdORSTfz/cVTnVtgz1C2BiT8HxLKiKSDNwHXAKUAdeISFm73W4E9qnqBGAhcJc7tgyYD0wF5gL3i0hyF22+A3wW2OLXe+rIoaPN7KhvsplfxpiE52dPZTZQpaoBVT0KLAbmtdtnHrDILT8DzBHv+tE8YLGqHlHVTUCVa6/DNlV1papu9vH9dChgNb+MMQbwN6kUAttCXm9368Luo6rNQD2Q18mxkbTZ5wJWndgYY4AEHKgXkZtEpFJEKoPBYFTatEKSxhjj8TOp1ABjQl4XuXVh9xGRFCAH2NPJsZG02SlVfVBVy1W1vKCgoDuHdqjaCkkaYwzgb1JZAUwUkRIRScMbeK9ot08FcJ1bvgpYoqrq1s93s8NKgInA8gjb7HOBYIONpxhjDD4mFTdGcjPwMvAR8JSqrhWRO0TkCrfbQ0CeiFQBtwC3umPXAk8B64CXgAWq2tJRmwAi8k0R2Y7Xe1ktIr/1672Fai0kaeMpxhgDKX42rqovAi+2W3d7yHITcHUHx94J3BlJm279PcA9vQy526yQpDHGnJBwA/XRdmI6sfVUjDHGkkovVQddIUnrqRhjjCWV3goEGxicnmKFJI0xBksqvVbtBumtkKQxxlhS6bVA0ApJGmNMK0sqvdBaSNLGU4wxxmNJpRdaZ37ZdGJjjPFYUumF1kKSpcPt8pcxxoAllV6prvUKSRbnWVIxxhiwpNIrgbpGioZZIUljjGllSaUXvEcI23iKMca0sqTSQ8ePK5vqrJCkMcaEsqTSQztdIUmbTmyMMSdYUumhgKv5ZT0VY4w5wZJKD7XeozLBeirGGNPGkkoPVbtCkgVWSNIYY9pYUumhQLCRUiskaYwxJ7Gk0kPVwQYrz2KMMe1YUumBQ0eb2VnfZNWJjTGmHUsqPdD2COHh1lMxxphQviYVEZkrIutFpEpEbg2zPV1EnnTbl4lIcci229z69SJycVdtikiJa6PKtZnm1/uqtunExhgTlm9JRUSSgfuAS4Ay4BoRKWu3243APlWdACwE7nLHlgHzganAXOB+EUnuos27gIWurX2ubV8Ego1WSNIYY8Lws6cyG6hS1YCqHgUWA/Pa7TMPWOSWnwHmiDedah6wWFWPqOomoMq1F7ZNd8yFrg1cm1/w641VBxuskKQxxoSR4mPbhcC2kNfbgU91tI+qNotIPZDn1r/X7thCtxyuzTxgv6o2h9n/JCJyE3ATwNixY7v3jpxTRmVTNGxQj441xpiBzM+kEpdU9UHgQYDy8nLtSRsLLpgQ1ZiMMWag8PPyVw0wJuR1kVsXdh8RSQFygD2dHNvR+j3AUNdGR+cyxhjjMz+TygpgopuVlYY38F7Rbp8K4Dq3fBWwRFXVrZ/vZoeVABOB5R216Y55w7WBa/M5H9+bMcaYMHy7/OXGSG4GXgaSgYdVda2I3AFUqmoF8BDwmIhUAXvxkgRuv6eAdUAzsEBVWwDCtelO+X1gsYj8BFjp2jbGGNOHxPsjPzGVl5drZWVlrMMwxph+RUTeV9XycNvsjnpjjDFRY0nFGGNM1FhSMcYYEzWWVIwxxkRNQg/Ui0gQ2NLDw/OBuiiGEy0WV/dYXN1jcXXPQI1rnKoWhNuQ0EmlN0SksqPZD7FkcXWPxdU9Flf3JGJcdvnLGGNM1FhSMcYYEzWWVHruwVgH0AGLq3ssru6xuLon4eKyMRVjjDFRYz0VY4wxUWNJxRhjTNRYUukBEZkrIutFpEpEbu2D820WkQ9FZJWIVLp1uSLyqohsdN+HufUiIve42FaLyKyQdq5z+28Ukes6Ol8XsTwsIrUisiZkXdRiEZHT3XutcsdKL+L6kYjUuM9tlYhcGrLtNneO9SJyccj6sD9b97iFZW79k+7RC13FNEZE3hCRdSKyVkS+FQ+fVydxxfrzyhCR5SLygYvrx521Jd6jMZ5065eJSHFP4+1hXL8TkU0hn9cMt77P/t27Y5NFZKWIPB8Pnxeqal/d+MIruV8NjAfSgA+AMp/PuRnIb7fup8CtbvlW4C63fCnwF0CAM4Flbn0uEHDfh7nlYT2I5dPALGCNH7HgPTfnTHfMX4BLehHXj4B/DrNvmfu5pQMl7ueZ3NnPFngKmO+WHwC+HkFMo4BZbnkIsMGdO6afVydxxfrzEmCwW04Flrn3FrYt4BvAA255PvBkT+PtYVy/A64Ks3+f/bt3x94CPAE839ln31efl/VUum82UKWqAVU9CiwG5sUgjnnAIre8CPhCyPpH1fMe3hMxRwEXA6+q6l5V3Qe8Cszt7klV9a94z76JeixuW7aqvqfev/ZHQ9rqSVwdmQcsVtUjqroJqML7uYb92bq/Gi8EngnzHjuLaaeq/t0tHwQ+AgqJ8efVSVwd6avPS1W1wb1MdV/aSVuhn+MzwBx37m7F24u4OtJn/+5FpAi4DPite93ZZ98nn5clle4rBLaFvN5O5/8ho0GBV0TkfRG5ya0boao73fIuYEQX8fkZd7RiKXTL0YzxZncJ4mFxl5l6EFcesF9Vm3sal7vUMBPvr9y4+bzaxQUx/rzcpZxVQC3eL93qTtpqO7/bXu/OHfX/A+3jUtXWz+tO93ktFJH09nFFeP7e/Bx/CXwPOO5ed/bZ98nnZUmlfzhXVWcBlwALROTToRvdXzdxMTc8nmIB/gsoBWYAO4FfxCIIERkM/AH4tqoeCN0Wy88rTFwx/7xUtUVVZwBFeH8pT+nrGMJpH5eITANuw4vvDLxLWt/vy5hE5HKgVlXf78vzdsWSSvfVAGNCXhe5db5R1Rr3vRb4I95/tt2u24z7XttFfH7GHa1YatxyVGJU1d3ul8Fx4Dd4n1tP4tqDdwkjpd36LolIKt4v7sdV9Vm3OuafV7i44uHzaqWq+4E3gLM6aavt/G57jju3b/8HQuKa6y4jqqoeAR6h559XT3+O5wBXiMhmvEtTFwJ3E+vPq6tBF/v6xKBYCt4AWwknBq+m+ni+LGBIyPK7eGMhP+Pkwd6fuuXLOHmQcLlbnwtswhsgHOaWc3sYUzEnD4hHLRY+OWB5aS/iGhWy/B2868YAUzl5YDKANyjZ4c8WeJqTBz+/EUE8gnd9/Jft1sf08+okrlh/XgXAULecCbwNXN5RW8ACTh54fqqn8fYwrlEhn+cvgf+Ixb97d/z5nBioj+3n1ZNfKon+hTe7YwPe9d5/9flc490P8wNgbev58K6Fvg5sBF4L+ccpwH0utg+B8pC2bsAbhKsCvtrDeH6Pd2nkGN411hujGQtQDqxxx/wKV/Whh3E95s67Gqjg5F+a/+rOsZ6QmTYd/Wzdz2G5i/dpID2CmM7Fu7S1Gljlvi6N9efVSVyx/rymAyvd+dcAt3fWFpDhXle57eN7Gm8P41riPq81wH9zYoZYn/27Dzn+fE4klZh+XlamxRhjTNTYmIoxxpiosaRijDEmaiypGGOMiRpLKsYYY6LGkooxxpiosaRiTDeJSF5IZdpdcnJl306r8YpIuYjc083z3eAq2K4WkTUiMs+tv15ERvfmvRgTbTal2JheEJEfAQ2q+vOQdSl6ovZSb9svAt7Cqypc70qrFKjqJhF5E6+qcGU0zmVMNFhPxZgocM/WeEBElgE/FZHZIrLUPefiXRGZ7PY7P+S5Fz9yhRvfFJGAiHwzTNPDgYNAA4CqNriEchXeDXOPux5Spnsmx1uu8OjLIaVg3hSRu91+a0RkdpjzGBMVllSMiZ4i4GxVvQX4GDhPVWcCtwP/t4NjpuCVRJ8N/JuryRXqA2A3sElEHhGRzwOo6jNAJfAV9QodNgP34j3f43TgYeDOkHYGuf2+4bYZ44uUrncxxkToaVVtccs5wCIRmYhXEqV9smj1gnoFCY+ISC1eGfy2Muiq2iIic/Eq4c4BForI6ar6o3btTAamAa96j8ggGa9sTavfu/b+KiLZIjJUveKIxkSVJRVjoqcxZPnfgTdU9R/cM0ve7OCYIyHLLYT5P6newOdyYLmIvIpXEfdH7XYTYK2qntXBedoPntpgqvGFXf4yxh85nCgTfn1PGxGR0RLyjHO8Z51sccsH8R4HDF4hwAIROcsdlyoiU0OO+7Jbfy5Qr6r1PY3JmM5YT8UYf/wU7/LXD4AXetFOKvBzN3W4CQgCX3Pbfgc8ICKH8Z47chVwj4jk4P3f/iVeZWuAJhFZ6dq7oRfxGNMpm1JszABnU49NX7LLX8YYY6LGeirGGGOixnoqxhhjosaSijHGmKixpGKMMSZqLKkYY4yJGksqxhhjoub/A2wc9nSuF+XGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Optimizor:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "d_model=512\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "    \n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "        \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "    \n",
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)\n",
    "# Learning rate curve:\n",
    "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
    "\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function:\n",
    "# loss and metric\n",
    "\n",
    "# For now I use sparse-cross entropy. But MAE may make more sense here:\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "  \n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "def create_masks(inp, tar):\n",
    "    # Encoder padding mask\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Used in the 2nd attention block in the decoder.\n",
    "    # This padding mask is used to mask the encoder outputs.\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Used in the 1st attention block in the decoder.\n",
    "    # It is used to pad and mask future tokens in the input received by \n",
    "    # the decoder.\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "      \n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "batch = 8\n",
    "\n",
    "transformer = Transformer(\n",
    "    num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
    "    input_seq_size=1000, output_seq_size=1000, \n",
    "    input_delta_t=1440, output_delta_t=240)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "checkpoint_path = \"./checkpoints/train_TS\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')\n",
    "\n",
    "\n",
    "\n",
    "train_step_signature = [\n",
    "        tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "        tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    ]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "\n",
    "def train_step(inp, tar):\n",
    "        \n",
    "    tar_inp = tar\n",
    "    tar_real = tar\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp, True, enc_padding_mask, combined_mask, dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing 0 (1795) batch in epoch 0 \n",
      "Loss tf.Tensor(7.3270817, shape=(), dtype=float32) Accuracy tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "Doing 200 (1795) batch in epoch 0 \n",
      "Loss tf.Tensor(6.7499447, shape=(), dtype=float32) Accuracy tf.Tensor(0.012437811, shape=(), dtype=float32)\n",
      "Doing 400 (1795) batch in epoch 0 \n",
      "Loss tf.Tensor(5.281816, shape=(), dtype=float32) Accuracy tf.Tensor(0.24522029, shape=(), dtype=float32)\n",
      "Doing 600 (1795) batch in epoch 0 \n",
      "Loss tf.Tensor(3.950485, shape=(), dtype=float32) Accuracy tf.Tensor(0.4592693, shape=(), dtype=float32)\n",
      "Doing 800 (1795) batch in epoch 0 \n",
      "Loss tf.Tensor(3.0790226, shape=(), dtype=float32) Accuracy tf.Tensor(0.58598626, shape=(), dtype=float32)\n",
      "Doing 1000 (1795) batch in epoch 0 \n",
      "Loss tf.Tensor(2.5135214, shape=(), dtype=float32) Accuracy tf.Tensor(0.66496, shape=(), dtype=float32)\n",
      "Doing 1200 (1795) batch in epoch 0 \n",
      "Loss tf.Tensor(2.1059744, shape=(), dtype=float32) Accuracy tf.Tensor(0.7201464, shape=(), dtype=float32)\n",
      "Doing 1400 (1795) batch in epoch 0 \n",
      "Loss tf.Tensor(1.8268478, shape=(), dtype=float32) Accuracy tf.Tensor(0.7582233, shape=(), dtype=float32)\n",
      "Doing 1600 (1795) batch in epoch 0 \n",
      "Loss tf.Tensor(1.6029106, shape=(), dtype=float32) Accuracy tf.Tensor(0.7882313, shape=(), dtype=float32)\n",
      "Doing 0 (1795) batch in epoch 1 \n",
      "Loss tf.Tensor(0.007809225, shape=(), dtype=float32) Accuracy tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "Doing 200 (1795) batch in epoch 1 \n",
      "Loss tf.Tensor(0.026332466, shape=(), dtype=float32) Accuracy tf.Tensor(0.99782336, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#Train and save:\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False)\n",
    "\n",
    "EPOCHS = 10\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train,y_train))\n",
    "batch=16\n",
    "\n",
    "N = len(y_train)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "  \n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    for i in range(N//batch):\n",
    "        inp, tar=X_train[batch*i:min(batch*i+batch,N),:,0],y_train[batch*i:min(batch*i+batch,N)]\n",
    "        tar = np.atleast_2d(tar)\n",
    "        lo = train_step(inp, tar)\n",
    "        if i%200==0:\n",
    "            print(\"Doing %d (%d) batch in epoch %d \"%(i,N//batch,epoch))\n",
    "            print(\"Loss\",train_loss.result(), \"Accuracy\",train_accuracy.result())\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing:\n",
    "N_test = len(y_test)\n",
    "\n",
    "y_pred_all = []\n",
    "for i in range(N_test//batch):\n",
    "    if i%200==0:\n",
    "            print(\"Doing %d (%d)\"%(i,N_test//batch))\n",
    "    \n",
    "    inp, tar=X_test[batch*i:min(batch*i+batch,N),:,0],y_test[batch*i:min(batch*i+batch,N)]\n",
    "    tar = np.atleast_2d(tar).T\n",
    "    tar_inp = tar\n",
    "    tar_real = tar\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "    \n",
    "    predictions, attention_weights = transformer(inp, \n",
    "                                                 tar,\n",
    "                                                 False,\n",
    "                                                 enc_padding_mask,\n",
    "                                                 combined_mask,\n",
    "                                                 dec_padding_mask)\n",
    "    y_pred_all.append(tf.cast(tf.argmax(predictions, axis=-1), tf.int32))\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot:\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib.pylab import rc\n",
    "\n",
    "\n",
    "font = {'family': 'normal','weight': 'bold',\n",
    "        'size': 25}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "rc('axes', linewidth=3)\n",
    "\n",
    "plt.plot(y_test[:3000],\"k\",label=\"Data\")\n",
    "plt.plot(y_pred_all[:3000],\"r\",label=\"Prediction-Transformer\")\n",
    "diff = y_test[:3000]-y_pred_all[:3000]\n",
    "plt.plot(diff,\"b\",label=\"Difference\")\n",
    "\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(r\"CIF\")\n",
    "plt.suptitle(\"Value vs day\")\n",
    "\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "plt.legend()\n",
    "\n",
    "fig.set_size_inches(35,12)\n",
    "\n",
    "save_path = plot_path + \"Transformer_CIF_short\" + \".png\"\n",
    "\n",
    "fig.savefig(save_path, dpi=200)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "font = {'family': 'normal','weight': 'bold',\n",
    "        'size': 25}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "rc('axes', linewidth=3)\n",
    "\n",
    "plt.hist(diff/np.nanmean(y_test))\n",
    "plt.xlabel(\"Difference/mean\")\n",
    "\n",
    "\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "plt.legend()\n",
    "\n",
    "fig.set_size_inches(35,12)\n",
    "\n",
    "save_path = plot_path + \"Transformer_CIF_diff_short_hist\" + \".png\"\n",
    "\n",
    "fig.savefig(save_path, dpi=200)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do some lone term prediction like 60 minutes?\n",
    "\n",
    "# You need to re-train the model since this model is only for short-term."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
